{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "# data preprocessing \n",
    "dataset=pd.read_csv('new_appdata100.csv')\n",
    "\n",
    "# first thing to do is to split the response variable from the independent features\n",
    "response = dataset[\"enrolled\"]\n",
    "# remove the response variable from the original dataset\n",
    "dataset= dataset.drop(columns ='enrolled')\n",
    "\n",
    "# now lets split the data into training and test data\n",
    "# we import the split data function from sklearn \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# it returns 4 variables two for training and 2 for testing \n",
    "# lets create them \n",
    "X_train ,  X_test ,y_train, y_test= train_test_split(dataset,response,\n",
    "                                             test_size=0.2,\n",
    "                                                 random_state=0 )\n",
    "# random state allows to duplicate the random selection in other occasions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice we have a user id in the data , and that variable is not helpful cause it is just an identifier of the user\n",
    "# so we are going to remove it from the data set\n",
    "# but we need to keep in mind that we need to assiciate the prediction we get with the user it came from \n",
    "\n",
    "train_identifier= X_train['user'] # id identifier for user in the train part\n",
    "X_train= X_train.drop(columns=['user'])\n",
    "\n",
    "# same thing for test identifier \n",
    "test_identifier=X_test['user']\n",
    "X_test=X_test.drop(columns =['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step : feature Scaller\n",
    "# we import that from sklearn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# important : StandardScaler return a numpy array of multiple dimensions \n",
    "# the problem with the process is that it losses the column name and also the index \n",
    "# and we need the index to identify which row of data we are dealing with\n",
    "sc_X= StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store the scale result into another data frame , that way we are able to compare it \n",
    "# with the original\n",
    "# we use fit_transform , it fit the data into the scaller \n",
    "X_train2= pd.DataFrame(sc_X.fit_transform(X_train))\n",
    "\n",
    "# we do the same with X_test\n",
    "X_test2= pd.DataFrame(sc_X.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now we are going to set the columns of the dataset into the orginal \n",
    "X_train2.columns = X_train.columns.values\n",
    "X_test2.columns =X_test.columns.values\n",
    "\n",
    "# now we need to recover the indexes of the orginal data\n",
    "X_train2.index = X_train.index.values\n",
    "X_test2.index =X_test.index.values\n",
    "\n",
    "X_train = X_train2\n",
    "X_test = X_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Building\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, penalty='l1')\n",
    "classifier.fit(X_train,y_train)\n",
    "# the model is fitted \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can predict \n",
    "y_pred= classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets evaluate our model \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score,precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7661460957178842"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred) # to see how many prediction we have right \n",
    "accuracy_score(y_test,y_pred) # the accuracy is 76% which is not bad\n",
    "# to make sure that this isnt happening because of some overfitting \n",
    "# let us see the precision : the precision is to see how many real positive we have in comparaision with true postive and false positive \n",
    "precision_score(y_test,y_pred)\n",
    "# the precision is also 76% which is as precise as the accuracy \n",
    "# which is a good sign \n",
    "\n",
    "# now the recall \n",
    "# it is defined as true positives divided by the sum of true positives and false negatives \n",
    "\n",
    "recall_score(y_test,y_pred) # 77% which also good \n",
    "\n",
    "f1_score(y_test,y_pred) # 76% also good \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Accuracy: 0.7679\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGoCAYAAABG5e9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xu8V1P++PHXKV1UUoooJdeFUYQGUVO5X8YtRuMy7oxmkHtCUUkk436X3EnuP7kPYXxnXIuK5RYKoZuuCufz++PzqTmdfU6nvt9P52Tv1/Px+DzO5+y19tprzzwezrv3e629S3K5HJIkSVlQq6YnIEmSVF0MfCRJUmYY+EiSpMww8JEkSZlh4CNJkjLDwEeSJGWGgY8kScoMAx9JkpQZBj6SJCkzDHwkSVJmGPhIkqTMMPCRJEmZsVpNXvznaZ/7hlSpmjVp072mpyBl1rz5X5RU5/WK+Xe2TvONqnXuK4sZH0mSlBk1mvGRJEkrUemvNT2DVY6BjyRJaZUrrekZrHIsdUmSpMww4yNJUlqVmvEpz8BHkqSUylnqSrDUJUmSMsOMjyRJaWWpK8HAR5KktLLUlWCpS5IkZYYZH0mS0soHGCYY+EiSlFaWuhIsdUmSpMww4yNJUlq5qyvBwEeSpJTyAYZJlrokSVJmmPGRJCmtLHUlGPhIkpRWlroSLHVJkqTMMOMjSVJa+QDDBAMfSZLSylJXgqUuSZKUGWZ8JElKK3d1JRj4SJKUVpa6Eix1SZKkzDDjI0lSWlnqSjDwkSQppXI5t7OXZ6lLkiRlhhkfSZLSysXNCQY+kiSllWt8Egx8JElKKzM+Ca7xkSRJmWHGR5KktPIlpQkGPpIkpZWlrgRLXZIkKTPM+EiSlFbu6kow8JEkKa0sdSVY6pIkSZlhxkeSpLSy1JVg4CNJUloZ+CRY6pIkSZlhxkeSpJTK5XyAYXkGPpIkpZWlrgRLXZIkKTPM+EiSlFY+xyfBwEeSpLSy1JVg4CNJkooqhNAKuBLYHagPjAHOjTFOKLQ/APQsd9rXMcb1C+21gP7ACUBT4HWgV4zx0zLX2Aa4GugITAeujTFeWdXcXOMjSVJa5UqL91lOIYQSYDSwPrAn+cBkAfBSCKFRoVt7oB+wXplPhzLD9ANOAU4EdgR+AZ4LIdQvXKM58CLwCbA9cBEwIIRwYlXzM+MjSVJa1UypqwXwIdAvxvgxQAhhIDAWaBdCeAfYDHgrxji1/MkhhHrAWcB5McbRhWM9gW+BQ4F7gJOARcApMcZfgA9DCJsAfYDbljU5Mz6SJKloYoxTY4w9ywQ9LYCzgW+AD4AtySdeJlYyxDZAI+DlMmPOBt4FuhQOdQZeKwQ9i70CbFQos1XKjI8kSWlVxF1dIYQmQJMKmmbFGGdVcs4I4GhgIbB/jHFuCKEd+dLV+SGEvQvfRwMXxRh/BBYHLlPKDfcN0LrwvRXJwOmbws/WwNeV3YcZH0mS0qq0tHgf6A1MquDTexkzGEp+jc8DwOMhhO2ArQptk4D9gHOAfYEnC4uaGxTaF5YbayH5hdIU+lTUTpk+FTLjI0mSlsfVwIgKjleY7QEos4vreGAH4DTgWOCyMlmi8SGEqcAb5BcyLygcr0d+HQ9lfp9b+L6g8Dvl2inTp0IGPpIkpVURFzcXApVKg5zFQgjrAd2AB2KMucK5pSGECUCrGGNpBeO8X/jZBvi88L0lEMv0aQlMKHyfXPidcu2QLJEtxVKXJElpVQPb2ckHL/cBOy8+EEKoA2xLfvfVYyGEJ8qd8/vCzwnAOGA20LXM+Y0L548pHHoV2CWEUDaB0w34uKKdYmWZ8ZEkScX0FvkdWbeGEE4in925AGgGXEW+5PVACKEP8DCwOXADMCrG+AFACOF6YHChBDYJGEJ+wfIjhWsMB84FhocQhgDbAWcCvaqanBkfSZLSqriLm5dLoZTVg/zTlh8G3gTWAjrHGCfFGB8EjiD/5OYPgNuBR4G/lBmmH/nn8dxKfu1PCbBXjHFR4RrfA3sAm5Df5j4Q6BNjHFHV/Epyudxy30yx/Tzt85q7uJRRTdp0r+kpSJk1b/4XJdV5vQVPXFG0v7OrH3Butc59ZTHjI0mSMsM1PpIkpZVvZ08w8JEkKa2K+OTmtLDUJUmSMsOMjyRJaWWpK8HAR5KktDLwSbDUJUmSMsOMjyRJaVWDz+pbVRn4SJKUVpa6Eix1SZKkzDDjI0lSWpnxSTDwkSQprXyAYYKlLkmSlBlmfCRJSitLXQkGPpIkpZXb2RMsdUmSpMww4yNJUlpZ6kow8JEkKa0MfBIsdUmSpMww4yNJUlr5HJ8EAx9JklIqV+qurvIsdUmSpMww4yNJUlq5uDnBwEeSpLRyjU+CpS5JkpQZZnwkSUorFzcnGPhIkpRWrvFJMPCRJCmtDHwSXOMjSZIyw4xPRo18fDT3Pvw4X3/zHeu1WJvDDt6PIw89gJKSEgA+/mwSV904nPcnfETdunXYcfsOnP2342nebC0ALhg0jCeeebHS8Z8bNYK33n2fCwdfVWmfQX3P5MB9dy/ujUm/Ieu1bMFbbz3PUUf24uWX/1Vhn/sfuJk5s+dy8slnL3W8ceM1uOiiM9ln391o3nwtxo4dz+BLr2bMmP9Zqt+7771ECBsnxt1llz/y3rsfFO9mtGrKucanPAOfDLrnoce44rrbOO6IQ/j9tlszbsJHDL3uNubNm89fjz2cH6bN4LhT+7DO2s24pE9vFi5cyNW3jOCkMy/koduvoU6dOpx0dE96/HHPpcadPWcu5/QfwrZbb8V6Ldamc6eO3H3j0MT1Bwy9njnz5tG5U8fqumVpldOq1Xo88eTdNG26ZoXttWrV4oqh/TjggL24955RS7WtttpqPP/8SDbcqA1Dh97AuLET6Nq1E489PoIjjujFM6NfAmD11euzySZtGTLkWl584dWlxvg4frZybkyrFktdCQY+GVNaWsrt9z7Mvnt044xTjgNg5x22Y/LX33LfqCf467GH89JrbzDrx9ncf+s/aLN+SwDWbLwGp5zdj3fGjWfH7TuwQetWbNC61VJj9+47iMZrNOKKi8+jVq1aNGvahGZNmyzV576Hn+DzLydz901XJtqkLCgpKeHwww/m0sF9qVWr4tUG7dptwZXDLqZDh3YsWPBTon3//fekXfstOPaY0xg58kkAXnhhDHXq1GHYsEuWBD5bbbUFtWvX5qmnnmfse+NX3k1JvyFVBj4hhLrAoUAXoDVQH5gLTAFeAR6NMf6yEueoIiopKeG2qy+lUcMGSx2vV7cOixb9DMDChYsAlurTtEn+X6WzfpxT4bhj3niTF8f8i2ED+7Jm4zUq7DNt+gyuvfUu/nTgPmyz1Rb/53uRfovatduCa68bzK233M0rr/yLRx8bkegzfPjVzJg5iy6d9+fJp+5JtG9WKF2NLgQ4i4159X/429+PY8stN2PixI9pv/WW/Pzzz3w48ZOVci/6DXA7e8IyFzeHfGF4InALsAUwG5gMzAe2AoYDH4QQ2q7caapYSkpK2GzjDWm5bgtyuRyzfpzNqCef5clnX6LnwX8EYO9d/0DzZk259Kqb+GHaDKZ8M5VhN9xB82ZN6fT7bRNjlpaWcuX1t7N9h3bs2b1zpde+/vZ7qFWrFqeddPRKuz9pVTd58te0a/cHzj//UubPT2ZzAI499nT23OMwPvro0wrbp0+fAcAGG6y/1PGNNmwDQNvCz/btt2TmjFncfPMVTJ4ylmnTP2LUI3ew8cZti3Q3WuXlSov3SYmqMj43AhHoEGNM/FM/hNAYeBC4Htiv+NPTyvTWu+9z3Gl9ANgybMpfeh4EwDprN6PfOadybv/Lee6f+XUBjddoxPBrh9B4jUaJcV5+/d9M+nIyfc84pdJrTZsxkyeffYlj/tyjwjGkrJg580dmzvxxmX3Gj/9ome1PPP4sl1xyLrfdfhWnntqXj+Nn7NJ5B07vfRIADRvks7Xt229J87WbMemLrzjsTyfStm1r+l7QmxdffJgdd9yH7777oTg3Jf2GVLWdfRfgvIqCHoAY42ygD/kymH5j2rRuyZ3XX85l/c5hzty5HHb8aUybMZOnn3+Z088fSJdOHbnlqkFcN6Q/m27UlpPOuIDPJn2ZGOf+UU+y+aYbsVPHDpVea9QTz1BamuPIQw9YmbckZcL330/jj388ijqrrcarrz7B1O/Gc/nlFzHgkisBmL9gAQBnndWf3XY9hAGXDOONN97i/vsf5cADjqbpWk3o1evYmrwFVZfSXPE+KVFVxmcm0ApY1qq4tsC8Yk1I1WfdddZm3XXWBqD9loF9e57AI089y5PPvET7LQPDBvZdsr19p993YP/DT+KaW+7i2iH9lowxY+Ys3h77Ab3/uuz/iD7/yut0+v22rOWCZqko3nl7HB077knLVuuyev36fPbZF3TvvgsAM2fMAuDdd95PnPfpp5P45OPPab/1ltU6X9WMnLu6EqoKfO4A7goh9Ce/kHkKsBCoB7QEugKDgdtW3hRVTLPnzGXMG2/Sod2WrN9y3SXHN2jdikYNGzD1u2l8M/U7uu68w5KgB6B+vXr8bvNNiZ9OWmq81/79Nr/+Wsoe3Spf2/Pt1O/5+NNJ/OVPBxX/hqQMatasKXvt1Z1nn/0n33w9dcnxDh3a8euvv/LBBx/SoMHq/OmwAxg7dnxiR1f91eszfdqM6p62tEqoqtR1MXAzMJT8IufZ5AOf2cBHheO3ABeuvCmq2C669CrufvDRpY6998FE5s6bz+abbsSGG7TmnXHjyZV58NVPCxcyMX66VLAE8P6Ej2ixdjNardei0uuNm5Bfr7Dt1r8r4l1I2VW7dm1uvW0YBx2875JjjRo15OhjDmPMK28wZ85cFiz4iYED+9Cv31lLnbt9x21o27Y1r776P+WHVRpZ6kpYZsYnxpgDLg4hDAa2IV/2agAsIL+7a1yMcdFKn6WKpvEajTj6zz248/5RNGjQgI4d2jHpqyncOuIBtthsYw7cZ3fWad6M084fwOnnD6TH/nuxaOEi7n34Cb77YTqX9TtnqfE++ewLNt5wg2Ve85PPvqBu3TpLngkk6f/m+++n8fDIJ+nX7ywWLVzI9z9M5+yze7HOOs058oheAORyOS4bfA1Dr+zPDTcO4fHHnmHDjdrQt29vxo4dz733PlLDd6FqkaLdWMWyXA8wLAQ3b67kuaianH7y0ay7TnMeeuxp7n7oUZo0bszeu3fl1BOPol69unTrvCM3XTmAm0c8wJkXXErDBqvzu8035cHbrmbzzZZ+9P30mbPYYrPk4/CX7jOTNRq5k0sqpr///XwGDupD/4vPYfXV6/Pmm++x1149ef/9iUv63Hjjnfz442z+furx/OlPBzBnzlweffRpBlxyJaWu/VBGleRq8D0eP0/7PD25M+k3okmb7jU9BSmz5s3/oqTqXkW83oAjivZ3tmG/+6p17iuLr6yQJCmtzOwlVLW4WZIkKTXM+EiSlFYp2o1VLAY+kiSllbu6Eix1SZKkzDDjI0lSWlnqSjDwkSQppXxXV5KlLkmSlBlmfCRJSitLXQkGPpIkpZWBT4KlLkmSlBlmfCRJSiuf45Ng4CNJUlpZ6kqw1CVJkjLDjI8kSSmVM+OTYOAjSVJa1VDgE0JoBVwJ7A7UB8YA58YYJxTatwGuBjoC04FrY4xXljm/FtAfOAFoCrwO9IoxflqmzzLHqIylLkmSVDQhhBJgNLA+sCf5wGQB8FIIoVEIoTnwIvAJsD1wETAghHBimWH6AacAJwI7Ar8Az4UQ6heusTxjVMiMjyRJaVUzr6xoAXwI9IsxfgwQQhgIjAXaAd2ARcApMcZfgA9DCJsAfYDbQgj1gLOA82KMowvn9wS+BQ4F7gFOWtYYy5qcGR9JktKqNFe8z3KKMU6NMfYsE/S0AM4GvgE+ADoDrxUClsVeATYqlMi2ARoBL5cZczbwLtClcKiqMSplxkeSJFUphNAEaFJB06wY46xKzhkBHA0sBPaPMc4tBCYTy3X9pvCzNdCy8H1KBX1aF75XNcbXld2HGR9JktKquBmf3sCkCj69lzGDoeTX+DwAPB5C2A5oQD4QKmvx7/UL7VTSp37he1VjVMqMjyRJKZXLFXVX19XAiAqOV5jtASizi+t4YAfgNPILneuV67r497mF9sXHFpXrM7fwvaoxKmXgI0mSqlQoZ1Ua5CwWQliP/ALmB2KMucK5pSGECeRLVJP5bzlrsbLlrVpljsVyfSYUvlc1RqUsdUmSlFY1sLgZaAPcB+y8+EAIoQ6wLfndXq8Cu4QQyiZfugEfxxinAuOA2UDXMuc3Lpw/pnCoqjEqZcZHkqS0qpkHGL5FfkfWrSGEk8hniS4AmgFXAfOAc4HhIYQhwHbAmUAvgBjjwhDC9cDgEMJU8uuIhpBfsPxI4RrDlzXGspjxkSRJRRNjLAV6kH/a8sPAm8BaQOcY46QY4/fAHsAm5LeoDwT6xBhHlBmmH/nn8dwKvAGUAHvFGBcVrrE8Y1SopMgLn1bIz9M+9yUiUjVr0qZ7TU9Byqx5878oqc7r/XjsbkX7O7vmnS9W69xXFktdkiSllS8pTbDUJUmSMsOMjyRJaVUjr+patRn4SJKUUjlLXQmWuiRJUmaY8ZEkKa3M+CQY+EiSlFau8Umw1CVJkjLDjI8kSSnl4uYkAx9JktLKUleCpS5JkpQZZnwkSUopS11JBj6SJKWVpa4EAx9JklIqZ+CT4BofSZKUGWZ8JElKKzM+CQY+kiSllKWuJEtdkiQpM8z4SJKUVmZ8Egx8JElKKUtdSZa6JElSZpjxkSQppcz4JBn4SJKUUgY+SZa6JElSZpjxkSQprXIlNT2DVY6BjyRJKWWpK8lSlyRJygwzPpIkpVSu1FJXeQY+kiSllKWuJEtdkiQpM8z4SJKUUjl3dSUY+EiSlFKWupIsdUmSpMww4yNJUkq5qyvJwEeSpJTK5Wp6BqseS12SJCkzzPhIkpRSlrqSDHwkSUopA58kS12SJCkzzPhIkpRSLm5OMvCRJCmlLHUlWeqSJEmZYcZHkqSU8l1dSQY+kiSllO/qSrLUJUmSMsOMjyRJKVVqqSvBwEeSpJRyjU+SpS5JkpQZZnwkSUopn+OTZOAjSVJK+eTmJEtdkiQpM8z4SJKUUpa6kgx8JElKKbezJ1nqkiRJmWHGR5KklPI5PkkGPpIkpZS7upIsdUmSpMww4yNJUkrV1OLmEMIawADgIKA58BEwIMb4ZKH9MqBPBafWiTH+UujzN+AsYD1gLHBajPGtMtdoC1wPdAEWACOACxafXxkzPpIkpVQuV1K0zwoaAewHnABsAzwKPBZC6F5obw/cRj6oWfIpE/QcA1wBXAhsB0TguRDCOoX2usDzQA7oBJwIHA9cUtXEzPhIkqSiCSGsCxwM7BdjfLFweHAIYVfywck/gXbAUzHGqZUM0xe4IcZ4f2HM44HPgJOBgcAhwAbADjHGmcD4EMJ5wDUhhEExxgWVzc+MjyRJKZXLFe+zAuYBewOvlp8OsFYIoQnQGphY0ckhhBbApsDLi4/FGH8FXiNf1gLoDIwtBD2LvQI0BLZd1uTM+EiSlFLFXONTCFiaVNA0K8Y4a/EvMcY5wLPlzt0R6A6cRj7bA3BECOEOoC75oKVPjPFboFWhfUq563wDdCx8b1VJO+SDqkrVaOCzesvONXl5KZPmfza6pqcg6bepN9C/guOXABdXdlIIYQvgMeA/wC3k1/0A/Aj0IL++ZzDwSgihA9Cg0L6w3FALgfqF7w2A7ytop0yfCpnxkSQppYr8AMOryS9aLm9WBccACCF0IR/0fAnsG2P8OYRwM/BgmTLV+yGE8cBk4EDyC5kB6pUbrh4wt/B9QSXtlOlTIQMfSZJSqpilrkI5q9Igp7wQwhHAcGAM0KNQAiPGmAPKrs0hxvh1CGE60AZ4oXC4JfBBmW4t+W95azLQodwlWxZ+li+BLcXFzZIkqahCCIcD9wAjyWd65pRpuyaE8F65/huSf97PhBjjD+SzPl3LtNcmv6B5TOHQq8A2hXVHi3UD5gDvLmtuZnwkSUqpmnhjRQhhffLP6HkZOBdoFkJY3LwIeBjoFUK4hvwDCFsC1wBvAk8X+g0Drg0hxMLxc4BGhXEBHgcGAQ+FEM4hv7X9MuCqGOOiZc3PjI8kSSlVmisp2mcFHEx+8XF38jutvi3zeTLG+Dr5hxv+HngPeAR4B9gnxlgKEGO8jfzDCwcCbwObAHvEGKcV2n8C9ipcb/Gi6VvIPy16mUpyNfgGs9XqtvL1aVI1c1eXVHPqtt66Wt8h8a91Dyna39mdp45KxavezfhIkqTMcI2PJEkpVVrTE1gFGfhIkpRSOVJRnSoqS12SJCkzzPhIkpRSpW4hSjDwkSQppUotdSVY6pIkSZlhxkeSpJRycXOSgY8kSSnldvYkS12SJCkzzPhIkpRSlrqSDHwkSUopS11JlrokSVJmmPGRJCmlzPgkGfhIkpRSrvFJstQlSZIyw4yPJEkpVWrCJ8HAR5KklPJdXUmWuiRJUmaY8ZEkKaVyNT2BVZCBjyRJKeV29iRLXZIkKTPM+EiSlFKlJS5uLs/AR5KklHKNT5KlLkmSlBlmfCRJSikXNycZ+EiSlFI+uTnJUpckScoMMz6SJKWUr6xIMvCRJCml3NWVZKlLkiRlhhkfSZJSysXNSQY+kiSllNvZkyx1SZKkzDDjI0lSSrm4OcnAR5KklHKNT5KlLkmSlBlmfCRJSikXNycZ+EiSlFIGPkmWuiRJUmaY8ZEkKaVyLm5OMPCRJCmlLHUlWeqSJEmZYcZHkqSUMuOTZOAjSVJK+eTmJEtdkiQpM8z4SJKUUr6yIsnAR5KklHKNT5KlLkmSlBlmfCRJSikzPkkGPpIkpZS7upIsdUmSpMww4yNJUkq5qyvJwEeSpJRyjU+SgY8kSSnlGp8k1/hIkqTMMOMjSVJKlZrzSTDwkSQppWpqjU8IYQ1gAHAQ0Bz4CBgQY3yy0N4WuB7oAiwARgAXxBh/KTPG34CzgPWAscBpMca3yrRXOUZFLHVJkqRiGwHsB5wAbAM8CjwWQugeQqgLPE9+CVIn4ETgeOCSxSeHEI4BrgAuBLYDIvBcCGGdQnuVY1TGwEeSpJTKFfGzvEII6wIHA71jjC/GGD+NMQ4GXiEfnBwCbAD8JcY4vpAFOg84PYSwemGYvsANMcb7Y4wTC+fNBk4utC/PGBUy8JEkKaVKi/hZAfOAvYFXyx3PAWsBnYGxMcaZZdpeARoC24YQWgCbAi8vbowx/gq8Rr6sRVVjLGtyrvGRJElVCiE0AZpU0DQrxjhr8S8xxjnAs+XO3RHoDpwG7AFMKTfGN4Wfrcmv16GSPh0L31tVMUalzPhIkpRSpSXF+wC9gUkVfHovaw4hhC2Ax4D/ALcADYCF5bot/r1+oZ1K+tQvfK9qjEqZ8ZEkKaWKvJ39avKLlsubVcExAEIIXcgHPV8C+8YYfw4hLADqleu6+Pe5/DfjU1GfuYXvVY1RKQMfSZJUpUI5q9Igp7wQwhHAcGAM0KNQAgOYDHQo171l4ecU4Ksyxz4o12dxeauqMSplqUuSpJSqiV1dACGEw4F7gJHkMz1zyjS/CmxTWDO0WDdgDvBujPEH8tvXu5YZrzb5Bc1jlmeMZc3NjI8kSSlVEw8wDCGsD9xGflfWuUCzEMLi5kXA48Ag4KEQwjnkt6VfBlwVY1xU6DcMuDaEEIE3gXOARoVxWc4xKmTGR5IkFdPB5Bcfdye/0+rbMp8nY4w/AXsV+i5e8HwL+Sc9AxBjvI38wwsHAm8DmwB7xBinFdqrHKMyJblczb3HY7W6rXyJiFTN5n82uqanIGVW3dZbl1Tn9c5r++ei/Z29/IsHqnXuK4ulLkmSUsrsQpKlLkmSlBlmfCRJSqmaejv7qszAR5KklCryAwxTwVKXJEnKDDM+kiSllPmeJAMfSZJSyjU+SZa6JElSZpjxkSQppXIWuxIMfCRJSilLXUmWuiRJUmaY8ZEkKaV8jk+SGZ+Ma9lyXX74bgK7du9caZ+HR97GHbf/I3G8adMmXP2PgXz80Rv8OPMT3n3nBU484chEv5123J6XXniYH2d+wpSv3uOO2//B2ms3K+p9SL8lI//fCxxw3Blsv88R/PGY07n30dGUfWH0x59/xV/PH8zOBx1Ltz+dxPlDrmPajFlLjfHrr6XccNdI9ji8F9vvcwR//tv5vPHO+4k+9z02moNOOIvf73cUex91KlfcdBfz5i+olvtUzcsV8ZMWBj4Ztv76LXn2mQdo2rRJhe21atXi6n8M5KAD90m01a5dmycfv4tDeuzHsKtu5tA/ncBLL77GDddfxqCBfZb067j9Nrzw/EMsWPATh/U8if4XD2Xvvbrz6KjhK+2+pFXZPY88zaBrbqfrTttz7YBz2af7Lgy9+S5uue8RAH6YPpPjz76EH6bP5JIz/8pZJx/F2+MmcnKfS/n551+WjHPFTSMY/uAT9DxgT4ZeeAZN12zM3y64jA8++nRJn+vufJArb7mHbp225x/9z+KIg/bmiedf4aTzBlFa6uoPZZOlrgwqKSnhyCMP4YohF1GrVsWxb/v2W3LNPway7bbtWbAg+a/D3XbtzE47bc9ee/fkxZdeA+D5F8awZpPGnHbqCQwYeBWLFi3i8iEXMm7cRPY/8Ogl/6H9cfYchl7ej7ZtW/PFF5NX3o1Kq5jS0lLuePBx9t11F8448QgAOm2/NZO/+Y77H3uGvx55CP/811vMmj2H+667lDat1gVgzTUa0avvZbzzwYfsuG07pn4/jZFPvUDvE4/g6EP2A6DLDtvSs1cfbrz7YW4afD4LflrIXaOe4qge+3LacX8GYOeO27BWk8acN/ha3ho3kR06bFUz/0Oo2ljqSjLjk0Ht22/JTTcM4d57R3HMsadV2Ofuu64DYMdO+zB9+qxE+8KFixhx10OMefXfSx2fOPFjGjRYnaZN12TttZuxyy47cOPNI5b61+WoUU+x4cYdDXqUOSUlJdx6+UWcemzPpY7XrVuHRYVszk+LFgHQqGGDJe1rrdkYgFmz5wDw7/fG88uvv7JH5x2X9Klduxa7d9mRf7+Qd2MJAAANB0lEQVT7PosW/czsufM4cM9u7PmHTktda+O2rQH4fvqMIt+dVkWlRfykhYFPBn311deELXbhnPMGML+SWv9Rf/k73XbtwYcfflJh+ytj3uCEE8/k559/Xur4/n/cg++++4Hvv5/G1u23pFatWkz7YTp333UdM6dHZs34mDuHX8Oahf+QS1lSUlLCZhu1oWWLtcnlcsz6cQ6PjH6Jp154lZ777wnA3l070XytJgy+7g5+mD6TKd9+z7Bb76X5Wk3otP3WAHz+1RTq16vLei2aLzV+m5br8ssvv/LVN1Np0Xwt+p9xEr/bbKOl+rz8xlsAbLZhm2q4Y2nVY6krg2bOnMXMmcksTlkffPDhCo/b+/ST6NJlJ07vfSG5XI7mhQXMt906jNHPvESPQ45n00034tJBfdh4ow3o0vXA/9X8pTR4a9wEjj97AABbbrYRR/XYF4B1mq/FRaefyHmDr+G5Mf8DQOM1GnLH0P40btQQgLnz5tOwweqJMRs2zB+bW8k/aMZN/Jg7HnicrjttT9i4bbFvSasgH2CYZOCjojjrzL9y+ZCLuOfeUdxw450A1K1TF4C33xnHX085F4B/vvw6s+fM4Z67rmf33brwwouv1ticpZrUptV6DB92MVN/mMZNdz9Mz159eODGy/jPu+M5f8h17NZ5Bw7ZZ1cWLvqZEQ8/xcl9BjF82MVsvMH6lJbmKCkpqXTsilreGjuB0/sPZf31WjDonF4r78a0SklTiapYqgx8QgivsZw72WKMXf7PM9JvSu3atbn+usGceMKR3DH8fk7pdd6Stjlz5wIwevRLS53z/POvALDNNlsZ+Ciz1l27GesWsqLtN9+U/Y45nUdH/5Mnnx9Du803YdhFZywJbnbarj0HHHcG19xxP9cOOJfGjRpWuCV93rz8sTXKrA8CePy5Vxhw9a1s0rY1Nw3uy5qNG63ku5NWXcuT8RkNDAI+At5cudPRb0mDBqvz6Kjh7LZbFwYMHMaAgVct1f7pp5MAqFev7lLH69SpA8CCBT9Vz0SlVcTsufMY8+936PC7zVl/vXWWHN9g/fVo1GB1pv4wjW++/4E/7LTdUhmd+vXq8rvNNiJ+/iUAbVu3ZMFPC/l+2gzWab7Wkn5ffv0tdevUYf31Wiw5dsNdI7n5nlHs3HEbrup3Jg1Wr18Nd6pVhaWupCoDnxjjZSGEH4EhwL4xxi9W+qy0yispKWHUyNvp2rUTxx1/BnffMzLRZ/z4j/jqq6/pediBS8pfAPvtuzsAr//rP9U2X2lV0W/oTRy63+70PfW4JcfGTojMnb+AzTduy4atW/HuBx+Sy/23nPXTwkVM/ORzNmjVEshvgS8pKeH5V//NkQfnn7P166+lvPjaf+i49ZbUrZv/x8UdDz7OzfeM4uC9u9Ov90nUru1+lqyx1JW0XGt8Yow3hhD2Jp/5ST6aV5lz3LF/Zo89unLf/Y/w2WeT2LlTx6Xa33p7HIsWLaJP30Hce/cNjHzoVu6443622GIzLrn4HB559GnGjp1QQ7OXakbjRg05+tA/cufIJ2jYoD4dt/4dkyZ/za33PcoWm2zIAXt2Ze1ma3F6/6Gc3n8oh+yzGwsXLeLex57hu2kzuKzPqUC+TNZjn10Zdus9zF/wE5tu2IZRT7/IZ19N4aLeJwLw5ZRvuf7Oh9iwTSv23+MPjJv48VJzabP+ujSv5OGlUpqtyOLmk4DtVtZE9NtySI/8Q9OOOLwHRxzeI9G+8aY78OWXUxg58knmz1/ABX1788ioO5gxYxY333wXF/W/orqnLK0STjuuJy3WXouRT73A3aOepknjRuzdbWf+fsxh1Ktbl26dtufGS8/nlvse4cwBw2jYYHW23GxjHrj+MjbfpO2ScfqeehxrNGrAg08+x5y589h0wzbcMKgP7bfYFICX/vUmv/z6K5O++ppjzuifmMfFZ55Mj312ra7bVg0pzVnqKq8kV4P/o6xWt5X/j0jVbP5no2t6ClJm1W29deXb8VaCIzc4uGh/Z+/98tFqnfvKYsFXkiRlhs/xkSQppXxXV5KBjyRJKeV29iRLXZIkKTPM+EiSlFI+xyfJwEeSpJRyjU+SpS5JkpQZZnwkSUopFzcnGfhIkpRSrvFJstQlSZIyw4yPJEkpVZOvpVpVGfhIkpRS7upKstQlSZIyw4yPJEkp5eLmJAMfSZJSyu3sSQY+kiSllGt8klzjI0mSMsOMjyRJKeV29iQDH0mSUsrFzUmWuiRJUmaY8ZEkKaXc1ZVk4CNJUkq5qyvJUpckScoMMz6SJKWUu7qSDHwkSUopS11JlrokSVJmmPGRJCml3NWVZOAjSVJKlbrGJ8FSlyRJygwzPpIkpZT5niQDH0mSUspdXUmWuiRJUmaY8ZEkKaVWhYxPCOF8YN8Y4y5ljl0G9Kmge50Y4y+FPn8DzgLWA8YCp8UY3yozRlvgeqALsAAYAVyw+PzKmPGRJCmlcrlc0T7/GyGEXsClFTS1B24jH9Qs+ZQJeo4BrgAuBLYDIvBcCGGdQntd4Hnyy5g6AScCxwOXVDUnMz6SJKmoQggtgVuAbuSDlvLaAU/FGKdWMkRf4IYY4/2F8Y4HPgNOBgYChwAbADvEGGcC40MI5wHXhBAGxRgXVDY3Mz6SJKVUKbmifVbQdsAc8pmd/5RtCCE0AVoDEys6MYTQAtgUeHnxsRjjr8Br5MtaAJ2BsYWgZ7FXgIbAtsuamBkfSZJSqqae3BxjfAp4CiCEUL65XeHnESGEO4C65IOWPjHGb4FWhfYp5c77BuhY+N6qknbIB1WVMvCRJElVKmRqmlTQNCvGOGsFhtqq8PNHoAf59T2DgVdCCB2ABoX2heXOWwjUL3xvAHxfQTtl+lTIwEeSpJT63y5KrkRvoH8Fxy8BLl6BcW4GHixTpno/hDAemAwcyH/XBNUrd149YG7h+4JK2inTp0IGPpIkpVSRt7NfTX7LeHkrku0hxpgDZpY79nUIYTrQBnihcLgl8EGZbi35b3lrMtCh3NAtCz/Ll8CWYuAjSZKqVChnrVCQU5EQwjVAlxhjhzLHNgSaAxNijD+EECLQFXiu0F6b/ILmWwqnvAocG0JoUqbM1o38gup3l3V9Ax9JklKqyKWuYnkY6FUIgK4nn6m5BngTeLrQZxhwbSEAehM4B2hE/tk/AI8Dg4CHQgjnkN/afhlwVYxx0bIu7nZ2SZJSqga3s1cqxvg6sB/we+A94BHgHWCfGGNpoc9t5B9eOBB4G9gE2CPGOK3Q/hOwV2HI/5DPBN0CDKjq+iU1GQ2uVrfVKhmKSmk2/7PRNT0FKbPqtt66pDqvt/W6nYr2d3bc1Deqde4ri6UuSZJSqqae47MqM/CRJCmlSlfNNT41yjU+kiQpM8z4SJKUUpa6kgx8JElKKUtdSZa6JElSZpjxkSQppSx1JRn4SJKUUpa6kix1SZKkzDDjI0lSSlnqSjLwkSQppSx1JVnqkiRJmWHGR5KklLLUlWTgI0lSSuVypTU9hVWOpS5JkpQZZnwkSUqpUktdCQY+kiSlVM5dXQmWuiRJUmaY8ZEkKaUsdSUZ+EiSlFKWupIsdUmSpMww4yNJUkr5yookAx9JklLKJzcnWeqSJEmZYcZHkqSUcnFzkoGPJEkp5Xb2JAMfSZJSyoxPkmt8JElSZpjxkSQppdzOnmTgI0lSSlnqSrLUJUmSMsOMjyRJKeWuriQDH0mSUspSV5KlLkmSlBlmfCRJSil3dSUZ+EiSlFK+pDTJUpckScoMMz6SJKWUpa4kAx9JklLKXV1JlrokSVJmmPGRJCmlXNycZOAjSVJKWepKstQlSZIyw4yPJEkpZcYnycBHkqSUMuxJKjEalCRJWeEaH0mSlBkGPpIkKTMMfCRJUmYY+EiSpMww8JEkSZlh4CNJkjLDwEeSJGWGgY8kScoMAx9JkpQZvrJCKySEUAvoD5wANAVeB3rFGD+t0YlJGRJCOB/YN8a4S03PRfqtMeOjFdUPOAU4EdgR+AV4LoRQv0ZnJWVECKEXcGlNz0P6rTLjo+UWQqgHnAWcF2McXTjWE/gWOBS4pwanJ6VaCKElcAvQDYg1PB3pN8uMj1bENkAj4OXFB2KMs4F3gS41NSkpI7YD5gDtgf/U8Fyk3ywzPloRrQo/p5Q7/g3QuprnImVKjPEp4CmAEEINz0b67TLjoxXRoPBzYbnjCwHX+EiSVnkGPloRCwo/65U7Xg+YW81zkSRphRn4aEVMLvxsWe54S5LlL0mSVjkGPloR44DZQNfFB0IIjYFtgTE1NCdJkpabi5u13GKMC0MI1wODQwhTgUnAEOBr4JEanZwkScvBwEcrqh9QG7gVaAi8BuwVY1xUo7OSJGk5lORyuZqegyRJUrVwjY8kScoMAx9JkpQZBj6SJCkzDHwkSVJmGPhIkqTMMPCRJEmZYeAjSZIyw8BHkiRlhoGPJEnKjP8PyU1/rWNiK1QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "print(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can see the positives prediction are very high compared to what we got wrong \n",
    "# true positives and false negatives we predicted are good (3802+3877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\python5\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Accuracy : 0.767 (+/- 0.010)\n"
     ]
    }
   ],
   "source": [
    "# now we are going to use a technique that will allows us to split the data into \n",
    "# multiple subset and we are going to test our model on each subset \n",
    "# this is an extra step to validate our result \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator= classifier, X=X_train, y=y_train, cv=10)\n",
    "print(\"Logistic Accuracy : %0.3f (+/- %0.3f)\" %(accuracies.mean(),accuracies.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see that the Logistic Accuracy : 0.767 (+/- 0.010) \n",
    "# so the standard deviation is 0.010 which is not that much \n",
    "# so the accuracy is also good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step is to assiciate the predicition with the proper user to see \n",
    "# if the user is going to enroll or not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>enrolled</th>\n",
       "      <th>predicted_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239786</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279644</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>237568</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65042</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>207226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>363062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>152296</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38108</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>359940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>136089</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>216038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18918</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>316730</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28308</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>228387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69640</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>358264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>348059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>178743</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>167556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>294101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>192801</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>163983</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>298830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>151790</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>348989</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>248593</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>316086</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>192540</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>256833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>273991</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>365937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>295129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>255715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>37332</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>164886</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>309967</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>14907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>244737</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>284862</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>60719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>262103</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>243679</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>280000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>255074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>347521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>335029</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>37271</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>240006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>279449</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>143036</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>91158</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>248318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>142418</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>279355</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  enrolled  predicted_results\n",
       "0     239786         1                  1\n",
       "1     279644         1                  1\n",
       "2      98290         0                  0\n",
       "3     170150         1                  1\n",
       "4     237568         1                  1\n",
       "5      65042         1                  0\n",
       "6     207226         1                  1\n",
       "7     363062         0                  0\n",
       "8     152296         1                  1\n",
       "9      64484         0                  0\n",
       "10     38108         1                  1\n",
       "11    359940         0                  0\n",
       "12    136089         0                  0\n",
       "13     14231         1                  1\n",
       "14    216038         0                  0\n",
       "15     18918         1                  1\n",
       "16    316730         1                  1\n",
       "17     28308         1                  0\n",
       "18    228387         1                  0\n",
       "19     69640         1                  1\n",
       "20    358264         0                  0\n",
       "21    348059         0                  0\n",
       "22    178743         1                  1\n",
       "23    167556         0                  0\n",
       "24    294101         0                  0\n",
       "25    192801         0                  1\n",
       "26    163983         1                  1\n",
       "27    298830         0                  0\n",
       "28    151790         1                  1\n",
       "29     20200         1                  1\n",
       "...      ...       ...                ...\n",
       "9970  348989         0                  1\n",
       "9971  248593         1                  0\n",
       "9972  316086         1                  1\n",
       "9973  192540         1                  1\n",
       "9974  256833         0                  0\n",
       "9975  273991         1                  1\n",
       "9976  365937         0                  0\n",
       "9977  295129         0                  0\n",
       "9978  255715         1                  0\n",
       "9979   37332         0                  1\n",
       "9980  164886         1                  1\n",
       "9981  309967         0                  1\n",
       "9982   14907         0                  0\n",
       "9983  244737         1                  1\n",
       "9984  284862         0                  1\n",
       "9985   60719         1                  1\n",
       "9986  262103         1                  0\n",
       "9987  243679         1                  1\n",
       "9988  280000         1                  1\n",
       "9989  255074         0                  0\n",
       "9990  347521         0                  0\n",
       "9991  335029         1                  0\n",
       "9992   37271         1                  0\n",
       "9993  240006         1                  1\n",
       "9994  279449         0                  1\n",
       "9995  143036         1                  0\n",
       "9996   91158         1                  1\n",
       "9997  248318         0                  0\n",
       "9998  142418         1                  1\n",
       "9999  279355         1                  1\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting the Final Results \n",
    "final_results = pd.concat([y_test,test_identifier],axis=1).dropna()\n",
    "final_results['predicted_results']= y_pred\n",
    "final_results[['user','enrolled','predicted_results']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
